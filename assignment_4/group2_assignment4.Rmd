---
title: "Group 2 Assignment 4"
date: "2024-11-11"
output: html_document
---

```{r setup, include=FALSE}
set.seed(24101968)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
                      error = FALSE, cache = TRUE, fig.height = 4, fig.width = 5,
                      fig.align = "center")
```

# Libraries

```{r}
library("cluster")
library("dbscan")
```

# Task 1

Reading in the data and looking at the head of it.

```{r}
rmf <- readRDS("rmf.rds")
head(rmf)
```

Performing a PCA on the dataset.

```{r}
rmf_pca <- princomp(scale(rmf))
```

Creating a barplot to see how many dimensions describe how much variance.

```{r}
sds <- rmf_pca$sdev
pve <- cumsum(sds^2) / sum(sds^2)
barplot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained")
abline(h = 0.95, lwd = 2, col = "red")
```

Plotting the biplot.

```{r}
biplot(rmf_pca)
```

Plotting the pairs plot.

```{r}
tmp <- (scale(rmf) %*% loadings(rmf_pca))[, 1:3]
opar <- par(no.readonly = TRUE)
par(mfrow = c(2, 2))
plot(tmp[, 1:2], asp = 1)
plot(tmp[, c(1, 3)], asp = 1)
plot(tmp[, 2:3], asp = 1)
par(opar)
```

# Task 2

## k-means

### elbow method

Doing the elbow method to decide how many clusters we should use. We will choose 4 means.

```{r}
kmcs <- lapply(ks <- 1:9, \(k) kmeans(rmf, nstart = 10, centers = k))
ss <- unlist(lapply(kmcs, \(obj) obj$tot.withinss))
plot(ks, ss, type = "b")
```

k-means method with 4 clusters and run it 10 times.

```{r}
rmf_kmc <- kmeans(rmf, centers = 4, nstart = 10)
scores <- rmf_pca$scores[, 1:2]
```

Plot the k-means model.

```{r}
plot(scores, pch = 20, xlim = 1.5 * range(scores[, 1]),
     ylim = 1.3 * range(scores[, 2]), col = rmf_kmc$cluster, asp = 1,
     main = "k-means clustering (4 clusters)")
text(scores, labels = rownames(rmf), adj = c(-0.1, 1), cex = 0.5,
     col = rmf_kmc$cluster)
```

## hierarchical clustering

Compute the Euclidean distance matrix of the data and ferform hierarchical clustering based on the distance matrix.

```{r}
rmf_dist <- dist(rmf, method = "euclidean")
rmf_clust <- hclust(rmf_dist)
```

### single method

Perform hierarchical clustering using the single linkage method.

```{r}
single <- hclust(rmf_dist, method = "single")
# cut the dendrogram tree into 4 clusters for single linkage
clust_single <- cutree(single, k = 4)
```

Plot the first two principal components and color points based on single linkage clusters.

```{r}
plot(scores, pch = 20, xlim = 1.5 * range(scores[, 1]),
     ylim = 1.3 * range(scores[, 2]), col = clust_single, asp = 1,
     main = "Single Linkage Clustering")
text(scores, labels = rownames(rmf), adj = c(-0.1, 1), cex = 0.5, col = clust_single)
```

### complete method

Perform hierarchical clustering using the complete linkage method.

```{r}
complete <- hclust(dist(rmf), method = "complete")
# cut the dendrogram tree into 4 clusters for complete linkage
clust_complete <- cutree(complete, k = 4)
```

Plot the first two principal components and color points based on complete linkage clusters.

```{r}
plot(scores, pch = 20, xlim = 1.5 * range(scores[, 1]),
     ylim = 1.3 * range(scores[, 2]), col = clust_complete, asp = 1,
     main = "Complete Linkage Clustering")
text(scores, labels = rownames(rmf), adj = c(-0.1, 1), cex = 0.5, col = clust_complete)
```

## DBSCAN

Apply the DBSCAN clustering algorithm to the scaled data.

```{r}
rmf_dbs <- dbscan(scale(rmf), eps = 0.5, minPts = 5)
```

Plot the first two principal components and coloring points by DBSCAN clusters.

```{r}
plot(scores, pch = 20, xlim = 1.5 * range(scores[, 1]), asp = 1,
    ylim = 1.3 * range(scores[, 2]), col = 1 + rmf_dbs$cluster,
    main = "DBSCAN")
```

When examining our four plots, we can conclude the following:

*k-means clustering with 4 clusters: k-means seems to keep the clusters well-separated and allows for a clear visualization.

*Single Linkage Clustering: results in a more compressed representation, making clusters less distinct.

*Complete Linkage Clustering: offers a middle option with better cluster separation than Single Linkage, but less clarity than k-means.

*DBSCAN: This clustering algorithm struggles significantly with this example. It cannot highlight a clear pattern."

# Task 3

After evaluating different clustering methods, we chose k-means clustering with k=4 based on the elbow method. K-means is suitable here because it minimizes within-cluster variance and provides distinct groupings based on the centroids of the clusters.
```{r}
kmeans_result <- kmeans(rmf, centers = 4)

rmf$cluster <- kmeans_result$cluster
```

To interpret the clusters, weâ€™ll calculate the average Recency, Frequency, and Monetary value for each cluster. This allows us to understand the "representative" customer in each group.
```{r}
avg_recency <- numeric(4)
avg_frequency <- numeric(4)
avg_monetary <- numeric(4)
cluster_size <- numeric(4)

#Loop through each cluster and calculate average values
for (i in 1:4) {
  cluster_data <- rmf[rmf$cluster == i, ] 
  
  avg_recency[i] <- mean(cluster_data$Recency, na.rm = TRUE)
  avg_frequency[i] <- mean(cluster_data$Frequency, na.rm = TRUE)
  avg_monetary[i] <- mean(cluster_data$Monetary, na.rm = TRUE)
  
  cluster_size[i] <- nrow(cluster_data)
}
```

At last, we loop through each cluster and print out the results.
```{r}
for (i in 1:4) {
  cat("\nCluster", i, "Summary:\n")
  cat("Average Recency:", avg_recency[i], "\n")
  cat("Average Frequency:", avg_frequency[i], "\n")
  cat("Average Monetary Value:", avg_monetary[i], "\n")
  cat("Number of Customers:", cluster_size[i], "\n")
}
```
Cluster 1 shows high-value, but infrequent buyers. These customers make large, infrequent purchases. Targeting them with exclusive deals or reminders to increase purchase frequency could lead to additional revenue.

Cluster 2 shows frequent, but low-Spending customers. This is the largest group, shopping frequently but spending little per transaction. Encouraging upsells or bundles could increase their overall value.

Cluster 3 shows moderate-value, but consistent buyers. These customers are moderately engaged, with balanced purchase patterns. Loyalty incentives could boost their spending and engagement further.

Cluster 4 shows inactive or low-value customers. This low-engagement, low-spending group may require re-engagement efforts, though they are likely lower priority.

This segmentation enables targeted marketing strategies for each group, optimizing customer engagement and maximizing value across different customer types.


